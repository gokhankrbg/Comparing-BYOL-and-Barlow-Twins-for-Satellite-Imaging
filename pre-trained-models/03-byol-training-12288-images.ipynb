{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":246010078,"sourceType":"kernelVersion"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# 03 - BYOL PRE-TRAINING (v4.1 - NUMERICAL STABILITY FIX)\n# ==============================================================================\n# Purpose: To train the BYOL model with enhanced numerical stability controls\n#          to prevent loss explosion.\n# Changes:\n#   - Added epsilon to F.normalize in the loss function.\n#   - Switched autocast to use bfloat16 for a wider dynamic range.\n#   - Automatically deletes old checkpoints on new runs to ensure a fresh start.\n# ==============================================================================\n\n# --- 0. Install and Import ---\n!pip install \"zarr>=2.10.0\" numcodecs -q\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T, torchvision.models as models\nfrom pathlib import Path\nimport numpy as np, zarr, random, math, os, copy, zipfile, tempfile, gc\nfrom tqdm.notebook import tqdm\n\nprint(\"Final BYOL Pre-training Script Initialized (Numerical Stability Fix).\")\n\n# --- 1. Configuration ---\nCONFIG = {\n    'data_path': \"/kaggle/input/01-data-preparation/data/ssl4eo-s12/train/S2RGB\",\n    'output_dir': \"/kaggle/working/\",\n    'epochs': 30,\n    'batch_size': 128,\n    'learning_rate': 1e-4,\n    'weight_decay': 1.5e-6,\n    'image_size': 224,\n    'projection_dim': 256,\n    'hidden_dim': 4096,\n    'base_tau': 0.996,\n    'num_workers': 2,\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n    'early_stopping_patience': 7,\n    'data_subset_percentage': 0.10,\n    'checkpoint_filename': \"byol_checkpoint_stable.pth\",\n    'best_model_filename': \"byol_best_encoder_stable.pth\"\n}\n\n# --- 2. Helper Classes & Functions ---\nclass EarlyStopping:\n    def __init__(self, patience=7, verbose=True, delta=0):\n        self.patience, self.verbose, self.delta = patience, verbose, delta\n        self.counter, self.best_score, self.early_stop, self.val_loss_min = 0, None, False, np.Inf\n    def __call__(self, val_loss, model, path):\n        if not math.isfinite(val_loss):\n            print(\"Loss is not finite, triggering early stop.\"); self.early_stop = True; return\n        score = -val_loss\n        if self.best_score is None: self.best_score = score; self.save_checkpoint(val_loss, model, path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose: print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience: self.early_stop = True\n        else: self.best_score = score; self.save_checkpoint(val_loss, model, path); self.counter = 0\n    def save_checkpoint(self, val_loss, model, path):\n        if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving best model...')\n        torch.save(model.online_encoder.state_dict(), path); self.val_loss_min = val_loss\n\ndef save_checkpoint(epoch, model, optimizer, scheduler, loss, config):\n    checkpoint_path = os.path.join(config['output_dir'], config['checkpoint_filename'])\n    state = {'epoch': epoch + 1, 'model_state_dict': model.state_dict(),\n             'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(), 'loss': loss}\n    torch.save(state, checkpoint_path)\n\ndef load_checkpoint(model, optimizer, scheduler, config):\n    checkpoint_path = os.path.join(config['output_dir'], config['checkpoint_filename'])\n    start_epoch = 0\n    if os.path.exists(checkpoint_path):\n        print(f\"Resuming from checkpoint: {checkpoint_path}\")\n        checkpoint = torch.load(checkpoint_path, map_location=config['device'])\n        model.load_state_dict(checkpoint['model_state_dict']); optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        scheduler.load_state_dict(checkpoint['scheduler_state_dict']); start_epoch = checkpoint['epoch']\n        print(f\"Resumed from epoch {start_epoch}, with previous loss {checkpoint['loss']:.4f}\")\n    else: print(\"No checkpoint found, starting from scratch.\")\n    return start_epoch\n\n# --- 3. Dataset, Model, and Loss ---\nclass SSL4EODataset(Dataset):\n    def __init__(self, root_dir, subset_percentage=1.0):\n        self.root_dir = Path(root_dir)\n        all_zarr_files = sorted(list(self.root_dir.glob(\"*.zarr.zip\")))\n        num_files_to_use = int(len(all_zarr_files) * subset_percentage)\n        self.zarr_files = random.sample(all_zarr_files, num_files_to_use) if subset_percentage < 1.0 else all_zarr_files\n        self.images = self._preload_images(); self.total_images = len(self.images)\n        print(f\"\\nDataset Initialized: Using {subset_percentage*100:.0f}% of data.\")\n        print(f\"-> Pre-loaded {self.total_images:,} images into memory.\")\n        self.transform_t = T.Compose([\n            T.ToPILImage(), T.RandomResizedCrop(CONFIG['image_size'], antialias=True), T.RandomHorizontalFlip(),\n            T.ColorJitter(0.4, 0.4, 0.2, 0.1), T.RandomGrayscale(p=0.2), T.GaussianBlur(23, (0.1, 2.0)),\n            T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n        self.transform_t_prime = T.Compose([\n            T.ToPILImage(), T.RandomResizedCrop(CONFIG['image_size'], antialias=True), T.RandomHorizontalFlip(),\n            T.ColorJitter(0.4, 0.4, 0.2, 0.1), T.RandomGrayscale(p=0.2), T.RandomSolarize(0.5, p=0.2),\n            T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n    def _preload_images(self):\n        images = []; print(\"Pre-loading images into RAM... This may take a few minutes.\")\n        for file_path in tqdm(self.zarr_files, desc=\"Loading files\"):\n            with tempfile.TemporaryDirectory() as temp_dir:\n                with zipfile.ZipFile(str(file_path), 'r') as zf: zf.extractall(temp_dir)\n                zarr_group = zarr.open(temp_dir, mode='r')\n                numpy_array = zarr_group['bands'][:]; images.extend(numpy_array.reshape(-1, *numpy_array.shape[2:]))\n        return images\n    def __len__(self): return self.total_images\n    def __getitem__(self, idx):\n        image_chw = self.images[idx]; image_hwc = np.transpose(image_chw, (1, 2, 0))\n        return self.transform_t(image_hwc), self.transform_t_prime(image_hwc)\n\nclass MLP(nn.Module):\n    def __init__(self, i, h, o): super().__init__(); self.net = nn.Sequential(nn.Linear(i, h), nn.BatchNorm1d(h), nn.ReLU(True), nn.Linear(h, o))\n    def forward(self, x): return self.net(x)\n\nclass BYOLModel(nn.Module):\n    def __init__(self, encoder, encoder_dim, p_dim, h_dim, b_tau):\n        super().__init__(); self.base_tau = b_tau\n        self.online_encoder=encoder; self.online_projector=MLP(encoder_dim,h_dim,p_dim); self.online_predictor=MLP(p_dim,h_dim,p_dim)\n        self.target_encoder=copy.deepcopy(encoder); self.target_projector=copy.deepcopy(self.online_projector)\n        for p in self.target_encoder.parameters(): p.requires_grad=False\n        for p in self.target_projector.parameters(): p.requires_grad=False\n    @torch.no_grad()\n    def update_target_network(self, cs, ts):\n        tau=1-(1-self.base_tau)*(math.cos(math.pi*cs/ts)+1)/2\n        for o,t in zip(self.online_encoder.parameters(), self.target_encoder.parameters()): t.data.mul_(tau).add_(o.data,alpha=1-tau)\n        for o,t in zip(self.online_projector.parameters(), self.target_projector.parameters()): t.data.mul_(tau).add_(o.data,alpha=1-tau)\n    def forward(self, v1, v2):\n        op1=self.online_predictor(self.online_projector(self.online_encoder(v1))); op2=self.online_predictor(self.online_projector(self.online_encoder(v2)))\n        with torch.no_grad(): tp1=self.target_projector(self.target_encoder(v1)); tp2=self.target_projector(self.target_encoder(v2))\n        return (op1, op2), (tp1.detach(), tp2.detach())\n\n# MODIFIED: Stabilized Loss Function\ndef byol_loss_fn(p, t):\n    p1, p2 = p; t1, t2 = t\n    eps = 1e-6\n    p1_norm = F.normalize(p1, p=2, dim=-1, eps=eps)\n    p2_norm = F.normalize(p2, p=2, dim=-1, eps=eps)\n    t1_norm = F.normalize(t1.detach(), p=2, dim=-1, eps=eps)\n    t2_norm = F.normalize(t2.detach(), p=2, dim=-1, eps=eps)\n    loss1 = 2 - 2 * (p1_norm * t2_norm).sum(dim=-1)\n    loss2 = 2 - 2 * (p2_norm * t1_norm).sum(dim=-1)\n    return (loss1 + loss2).mean() * 0.5\n\n\n# --- 4. Main Training Execution ---\nif __name__ == '__main__':\n    gc.collect(); torch.cuda.empty_cache()\n    device = torch.device(CONFIG['device']); print(f\"Using device: {device}\")\n    \n    dataset = SSL4EODataset(root_dir=CONFIG['data_path'], subset_percentage=CONFIG.get('data_subset_percentage', 1.0))\n    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n                        num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True)\n    \n    print(\"Using ResNet-18 as the encoder model.\")\n    resnet = models.resnet18(weights=None); encoder_output_dim = resnet.fc.in_features; resnet.fc = nn.Identity()\n    \n    model = BYOLModel(\n        encoder=resnet, encoder_dim=encoder_output_dim,\n        p_dim=CONFIG['projection_dim'], h_dim=CONFIG['hidden_dim'], b_tau=CONFIG['base_tau']\n    ).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    \n    total_steps = len(loader) * CONFIG['epochs']\n    warmup_steps = len(loader) * 1\n    def lr_lambda(current_step):\n        if current_step < warmup_steps: return float(current_step) / float(max(1, warmup_steps))\n        progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n        return 0.5 * (1.0 + math.cos(math.pi * progress))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    \n    scaler = torch.cuda.amp.GradScaler(enabled=(CONFIG['device'] == 'cuda'))\n    early_stopper = EarlyStopping(patience=CONFIG['early_stopping_patience'], verbose=True)\n    \n    checkpoint_path = os.path.join(CONFIG['output_dir'], CONFIG['checkpoint_filename'])\n    if os.path.exists(checkpoint_path):\n        print(f\"Deleting old checkpoint at {checkpoint_path} to start fresh.\"); os.remove(checkpoint_path)\n    start_epoch = 0\n    \n    print(f\"\\n--- Starting Training with Stability Controls on {CONFIG['data_subset_percentage']*100:.0f}% of Data ---\")\n    for epoch in range(start_epoch, CONFIG['epochs']):\n        model.train(); total_loss = 0.0\n        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n        for i, (view1, view2) in enumerate(pbar):\n            view1, view2 = view1.to(device), view2.to(device)\n            \n            # Use bfloat16 for better numerical stability with AMP\n            with torch.cuda.amp.autocast(enabled=(CONFIG['device'] == 'cuda'), dtype=torch.bfloat16):\n                predictions, targets = model(view1, view2)\n                loss = byol_loss_fn(predictions, targets)\n            \n            optimizer.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            scheduler.step()\n            current_step = epoch * len(loader) + i\n            model.update_target_network(current_step, total_steps)\n            \n            total_loss += loss.item()\n            pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'LR': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n\n        avg_loss = total_loss / len(loader)\n        if not math.isfinite(avg_loss):\n            print(f\"Epoch {epoch+1} ended with non-finite loss: {avg_loss}. Stopping training.\"); break\n            \n        print(f\"Epoch {epoch+1} Summary: Average Loss = {avg_loss:.4f}\")\n        save_checkpoint(epoch, model, optimizer, scheduler, avg_loss, CONFIG)\n        early_stopper(avg_loss, model, os.path.join(CONFIG['output_dir'], CONFIG['best_model_filename']))\n        if early_stopper.early_stop:\n            print(\"Early stopping triggered due to no improvement in loss.\"); break\n\n    print(\"\\n--- Training Finished ---\")\n    final_encoder_path = os.path.join(CONFIG['output_dir'], \"byol_final_encoder.pth\")\n    torch.save(model.online_encoder.state_dict(), final_encoder_path)\n    print(f\"Final online encoder saved to {final_encoder_path}\")\n    print(f\"Best encoder (lowest loss) saved to {os.path.join(CONFIG['output_dir'], CONFIG['best_model_filename'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T23:06:25.224948Z","iopub.execute_input":"2025-06-18T23:06:25.225567Z","iopub.status.idle":"2025-06-18T23:29:03.741358Z","shell.execute_reply.started":"2025-06-18T23:06:25.225534Z","shell.execute_reply":"2025-06-18T23:29:03.740243Z"}},"outputs":[{"name":"stdout","text":"Final BYOL Pre-training Script Initialized (Numerical Stability Fix).\nUsing device: cuda\nPre-loading images into RAM... This may take a few minutes.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading files:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77dda4854728400cb2e78aee8b1d6573"}},"metadata":{}},{"name":"stdout","text":"\nDataset Initialized: Using 10% of data.\n-> Pre-loaded 12,288 images into memory.\nUsing ResNet-18 as the encoder model.\nDeleting old checkpoint at /kaggle/working/byol_checkpoint_stable.pth to start fresh.\n\n--- Starting Training with Stability Controls on 10% of Data ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/2841839711.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(CONFIG['device'] == 'cuda'))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/30:   0%|          | 0/96 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b78c8f3aca74e988c338e0ea0e281dd"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_35/2841839711.py:189: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(CONFIG['device'] == 'cuda'), dtype=torch.bfloat16):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Summary: Average Loss = 0.9380\nValidation loss decreased (inf --> 0.937999). Saving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/30:   0%|          | 0/96 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"114f5ca108c44982a0276cbb871c0765"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 Summary: Average Loss = 0.6688\nValidation loss decreased (0.937999 --> 0.668840). Saving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/30:   0%|          | 0/96 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d2cf9f0f5a45af8905b598daaeef6d"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/2841839711.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}