{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd889fc6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-29T13:37:12.741999Z",
     "iopub.status.busy": "2025-06-29T13:37:12.741672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-06-29T13:37:12.738042",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/205.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m204.8/205.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/8.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/8.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/8.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m6.4/8.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pre-loading images into RAM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154e3f79ba6a4d4e82ee6c4784e643c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading files:   0%|          | 0/241 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Initialized: Using 50% of data. -> Pre-loaded 61,696 images.\n",
      "Using ResNet-18 as the encoder model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resuming from checkpoint: /kaggle/input/04-barlow-twins-training-50pct/barlow_checkpoint_50pct.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Resuming training from the start of epoch 17\n",
      "\n",
      "--- Resuming Barlow Twins Training from Epoch 17 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486a8638088349d9b5440b1f9275c4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Summary: Average Loss = 457.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 457.667400). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bed144b1c74ba095a52ccc4de0857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Summary: Average Loss = 446.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (457.667400 --> 446.412952). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf7bd5967c243f29234b144c701a344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Summary: Average Loss = 436.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (446.412952 --> 436.035797). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331176612f23437490c620f99c71ebee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Summary: Average Loss = 427.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (436.035797 --> 427.153399). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f66c0611684437be744358e8e3b250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Summary: Average Loss = 419.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (427.153399 --> 419.742676). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666d8d6812be4521af8bfb1a5e968238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Summary: Average Loss = 413.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (419.742676 --> 413.367056). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0631906c31e64619a3a8e0a4a64c740e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Summary: Average Loss = 407.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (413.367056 --> 407.925351). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb76f8552c214cc98fbcb7b6506f2e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Summary: Average Loss = 403.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (407.925351 --> 403.442369). Saving best model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4951dd05eb4649b6beef142db7a1e0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/30:   0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 04c - RESUME BARLOW TWINS TRAINING (PART 3 - FINAL PATH CORRECTION)\n",
    "# ==============================================================================\n",
    "# Purpose: To resume the Barlow Twins training from the correct checkpoint path.\n",
    "# Change: Updated 'checkpoint_input_folder' to match the actual Kaggle path.\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 0. Imports ---\n",
    "!pip install \"zarr>=2.10.0\" numcodecs -q\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, torchvision.transforms as T, torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import numpy as np, zarr, random, math, os, copy, zipfile, tempfile, gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "CONFIG = {\n",
    "    # Path to the original data from the preparation notebook\n",
    "    'data_input_folder': '01-data-preparation',\n",
    "    \n",
    "    # MODIFIED: Use the EXACT folder name as listed by 'ls -R'\n",
    "    'checkpoint_input_folder': '04-barlow-twins-training-50pct', \n",
    "    \n",
    "    # Where to save outputs for THIS run\n",
    "    'output_dir': \"/kaggle/working/\",\n",
    "    \n",
    "    # Training parameters must match the previous run to resume correctly\n",
    "    'epochs': 30, # The final target number of epochs\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1.5e-6,\n",
    "    'image_size': 224,\n",
    "    'projection_dim': 2048,\n",
    "    'hidden_dim': 4096,\n",
    "    'lambda_param': 5e-3,\n",
    "    'num_workers': 2,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'early_stopping_patience': 10,\n",
    "    'data_subset_percentage': 0.50,\n",
    "    \n",
    "    # Filenames must match exactly\n",
    "    'checkpoint_filename': \"barlow_checkpoint_50pct.pth\",\n",
    "    'best_model_filename': \"barlow_best_encoder_50pct.pth\"\n",
    "}\n",
    "\n",
    "# --- 2. Helper Classes & Functions ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=True, delta=0):\n",
    "        self.patience, self.verbose, self.delta = patience, verbose, delta\n",
    "        self.counter, self.best_score, self.early_stop, self.val_loss_min = 0, None, False, np.Inf\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        if not math.isfinite(val_loss): print(\"Loss is not finite, stopping.\"); self.early_stop = True; return\n",
    "        score = -val_loss\n",
    "        if self.best_score is None: self.best_score = score; self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose: print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience: self.early_stop = True\n",
    "        else: self.best_score = score; self.save_checkpoint(val_loss, model, path); self.counter = 0\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose: print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving best model...')\n",
    "        torch.save(model.encoder.state_dict(), path); self.val_loss_min = val_loss\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, loss, config):\n",
    "    state = {'epoch': epoch + 1, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'scheduler_state_dict': scheduler.state_dict(), 'loss': loss}\n",
    "    torch.save(state, os.path.join(config['output_dir'], config['checkpoint_filename']))\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, config):\n",
    "    path = Path(f\"/kaggle/input/{config['checkpoint_input_folder']}/{config['checkpoint_filename']}\")\n",
    "    start_epoch = 0\n",
    "    if path.exists():\n",
    "        print(f\"✅ Resuming from checkpoint: {path}\")\n",
    "        ckpt = torch.load(path, map_location=config['device'])\n",
    "        model.load_state_dict(ckpt['model_state_dict']); optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(ckpt['scheduler_state_dict']); start_epoch = ckpt['epoch']\n",
    "        print(f\"   -> Resuming training from the start of epoch {start_epoch}\")\n",
    "    else: print(f\"❌ Checkpoint not found at {path}. Starting a fresh run.\")\n",
    "    return start_epoch\n",
    "\n",
    "# --- 3. Dataset, Model, and Loss ---\n",
    "class SSL4EODataset(Dataset):\n",
    "    def __init__(self, root_dir, subset_percentage=1.0):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        all_zarr_files = sorted(list(self.root_dir.glob(\"*.zarr.zip\")))\n",
    "        num_files_to_use = int(len(all_zarr_files) * subset_percentage)\n",
    "        random.seed(42) # for reproducibility of the subset\n",
    "        self.zarr_files = random.sample(all_zarr_files, num_files_to_use) if subset_percentage < 1.0 else all_zarr_files\n",
    "        self.images = self._preload_images(); self.total_images = len(self.images)\n",
    "        print(f\"\\nDataset Initialized: Using {subset_percentage*100:.0f}% of data. -> Pre-loaded {self.total_images:,} images.\")\n",
    "        self.transform = T.Compose([\n",
    "            T.ToPILImage(), T.RandomResizedCrop(CONFIG['image_size'], antialias=True), T.RandomHorizontalFlip(),\n",
    "            T.ColorJitter(0.4, 0.4, 0.2, 0.1), T.RandomGrayscale(p=0.2), T.GaussianBlur(23, (0.1, 2.0)),\n",
    "            T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    def _preload_images(self):\n",
    "        images = []; print(\"Pre-loading images into RAM...\")\n",
    "        for fp in tqdm(self.zarr_files, desc=\"Loading files\"):\n",
    "            with tempfile.TemporaryDirectory() as td:\n",
    "                with zipfile.ZipFile(str(fp), 'r') as zf: zf.extractall(td)\n",
    "                za = zarr.open(td, mode='r')['bands'][:]; images.extend(za.reshape(-1, *za.shape[2:]))\n",
    "        return images\n",
    "    def __len__(self): return self.total_images\n",
    "    def __getitem__(self, idx):\n",
    "        image_chw = self.images[idx]; image_hwc = np.transpose(image_chw, (1, 2, 0))\n",
    "        view1 = self.transform(image_hwc); view2 = self.transform(image_hwc)\n",
    "        return view1, view2\n",
    "\n",
    "class BarlowTwinsModel(nn.Module):\n",
    "    def __init__(self, encoder, encoder_dim, projection_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(True),\n",
    "            nn.Linear(hidden_dim, projection_dim))\n",
    "    def forward(self, v1, v2):\n",
    "        z1 = self.projector(self.encoder(v1)); z2 = self.projector(self.encoder(v2))\n",
    "        return z1, z2\n",
    "\n",
    "def barlow_twins_loss_fn(z1, z2, lambda_param):\n",
    "    batch_size = z1.shape[0]\n",
    "    z1_norm = (z1 - z1.mean(dim=0)) / (z1.std(dim=0) + 1e-5)\n",
    "    z2_norm = (z2 - z2.mean(dim=0)) / (z2.std(dim=0) + 1e-5)\n",
    "    c = (z1_norm.T @ z2_norm) / batch_size\n",
    "    on_diag_loss = ((torch.diagonal(c) - 1)**2).sum()\n",
    "    off_diag = c.fill_diagonal_(0)\n",
    "    off_diag_loss = (off_diag**2).sum()\n",
    "    return on_diag_loss + lambda_param * off_diag_loss\n",
    "\n",
    "# --- 4. Main Training Execution ---\n",
    "if __name__ == '__main__':\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    device = torch.device(CONFIG['device']); print(f\"Using device: {device}\")\n",
    "    \n",
    "    data_full_path = Path(f\"/kaggle/input/{CONFIG['data_input_folder']}/data/ssl4eo-s12/train/S2RGB\")\n",
    "    dataset = SSL4EODataset(data_full_path, CONFIG['data_subset_percentage'])\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                        num_workers=CONFIG['num_workers'], pin_memory=True, drop_last=True)\n",
    "    \n",
    "    print(\"Using ResNet-18 as the encoder model.\")\n",
    "    resnet = models.resnet18(weights=None); encoder_output_dim = resnet.fc.in_features; resnet.fc = nn.Identity()\n",
    "    \n",
    "    model = BarlowTwinsModel(\n",
    "        encoder=resnet, encoder_dim=encoder_output_dim,\n",
    "        projection_dim=CONFIG['projection_dim'], hidden_dim=CONFIG['hidden_dim']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(loader)*CONFIG['epochs'])\n",
    "    \n",
    "    early_stopper = EarlyStopping(patience=CONFIG['early_stopping_patience'], verbose=True)\n",
    "    \n",
    "    # This will load the state from the previous 12-hour run\n",
    "    start_epoch = load_checkpoint(model, optimizer, scheduler, CONFIG)\n",
    "    \n",
    "    if start_epoch > 0:\n",
    "        print(f\"\\n--- Resuming Barlow Twins Training from Epoch {start_epoch} ---\")\n",
    "        for epoch in range(start_epoch, CONFIG['epochs']):\n",
    "            model.train(); total_loss = 0.0\n",
    "            pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\")\n",
    "            for view1, view2 in pbar:\n",
    "                view1, view2 = view1.to(device), view2.to(device)\n",
    "                z1, z2 = model(view1, view2)\n",
    "                loss = barlow_twins_loss_fn(z1, z2, lambda_param=CONFIG['lambda_param'])\n",
    "                \n",
    "                optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': f\"{loss.item():.2f}\", 'LR': f\"{optimizer.param_groups[0]['lr']:.6f}\"})\n",
    "\n",
    "            avg_loss = total_loss / len(loader)\n",
    "            print(f\"Epoch {epoch+1} Summary: Average Loss = {avg_loss:.2f}\")\n",
    "            \n",
    "            save_checkpoint(epoch, model, optimizer, scheduler, avg_loss, CONFIG)\n",
    "            early_stopper(avg_loss, model, os.path.join(CONFIG['output_dir'], CONFIG['best_model_filename']))\n",
    "            if early_stopper.early_stop:\n",
    "                print(\"Early stopping triggered.\"); break\n",
    "\n",
    "        print(\"\\n--- Training Finished ---\")\n",
    "        torch.save(model.encoder.state_dict(), os.path.join(CONFIG['output_dir'], \"barlow_final_encoder_50pct.pth\"))\n",
    "        \n",
    "    else:\n",
    "        print(\"Could not find a valid checkpoint to resume from. Halting execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119bf90",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 246582072,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 247898108,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-29T13:37:06.756081",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}